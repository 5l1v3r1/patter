{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: CTCLoss not imported. Use only for inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from patter import ModelFactory\n",
    "from patter.data import AudioSegment\n",
    "from patter.decoder import BeamCTCDecoder\n",
    "from patter.data.features import PerturbedSpectrogramFeaturizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths to Models/Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../../models/librispeech_pretrained_patter.pt\"\n",
    "lm_path = \"../../models/lm/3-gram.pruned.1e-7.bin\"\n",
    "audio_path = \"../../data/sample/1089-134691-0003.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "The ModelFactory class is responsible for reading a serialized model file, and initializing an instance\n",
    "of the correct type of model (e.g. DeepSpeech2 or its variants, Wav2Letter, etc). The optional keyword\n",
    "argument `include_package`, when True, returns a second object which is a dictionary of additional\n",
    "model metadata. It is not necessary if using the model for training or decoding. The contents of the\n",
    "dictionary are only useful for introspection into how the model was created and its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ModelFactory.load(\"../../models/librispeech_pretrained_patter.pt\")\n",
    "model, package = ModelFactory.load(\"../../models/librispeech_pretrained_patter.pt\", include_package=True)\n",
    "\n",
    "# put model in evaluation mode (crucial!)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSpeechOptim(\n",
      "  (conv): Sequential(\n",
      "    (0.cnn): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(0, 10))\n",
      "    (0.batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (0.act): Hardtanh(min_val=0, max_val=20)\n",
      "    (1.cnn): Conv2d(32, 32, kernel_size=(21, 11), stride=(2, 1))\n",
      "    (1.batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1.act): Hardtanh(min_val=0, max_val=20)\n",
      "  )\n",
      "  (rnn): DeepBatchRNN(\n",
      "    (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (rnns): Sequential(\n",
      "      (0): BatchRNN(\n",
      "        (rnn): GRU(672, 800, bias=False, bidirectional=True)\n",
      "      )\n",
      "      (1): BatchRNN(\n",
      "        (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "      )\n",
      "      (2): BatchRNN(\n",
      "        (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "      )\n",
      "      (3): BatchRNN(\n",
      "        (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "      )\n",
      "      (4): BatchRNN(\n",
      "        (batch_norm): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (rnn): GRU(800, 800, bias=False, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Linear(in_features=800, out_features=29, bias=False)\n",
      "  )\n",
      "  (inference_softmax): InferenceBatchSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Featurizer\n",
    "The model configuration includes what featurization is required to convert the audio to a format acceptable to it.\n",
    "A patter featurizer is responsible for reading that configuration, then reading in audio from its on-disk format\n",
    "and returning a tensor ready for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = PerturbedSpectrogramFeaturizer.from_config(model.input_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Decoder\n",
    "Setup a beam decoder with a language model that can be used to convert the acoustic model outputs to the final transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = BeamCTCDecoder(model.labels, lm_path=lm_path, alpha=2.15, beta=0.85, beam_width=100, blank_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe audio\n",
    "To transcribe audio, it must be loaded from disk and featurized. The input features are then passed through the\n",
    "acoustic model. The output of the acoustic model must then be decoded using a properly initialized decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = featurizer.process(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This adds two singleton dimensions to the feature tensor.\n",
    "# The model expects input in the form (1, batch_size, feature_size, max_seq_len)\n",
    "features = features.unsqueeze(0).unsqueeze(0)\n",
    "seq_len = torch.IntTensor([features.size(3)])\n",
    "\n",
    "# run model\n",
    "output, output_len = model(torch.autograd.Variable(features, volatile=True),\n",
    "                           torch.autograd.Variable(seq_len, volatile=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decode acoustic model output\n",
    "output = output.transpose(0, 1) # decoder requires [batch_size, seq_len, character_classes]\n",
    "transcript, offsets, scores = decoder.decode(output.data, output_len.data, num_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-rated final transcript for first utterance in batch:\n",
      "THE UNIVERSITY\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-rated final transcript for first utterance in batch:\")\n",
    "print(transcript[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "**Note:** If you want to run the above on a GPU, the model and Variables must be moved to the GPU, i.e.:\n",
    "\n",
    "```python\n",
    "model = model.cuda()\n",
    "\n",
    "features = torch.autograd.Variable(features, volatile=True).cuda()\n",
    "seq_len = torch.autograd.Variable(seq_len, volatile=True).cuda()\n",
    "```\n",
    "\n",
    "and the output will have to be moved back to the CPU prior to decoding, i.e.:\n",
    "\n",
    "```python\n",
    "output = output.cpu()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
