syntax = "proto3";

package parlance.patter.speech.v1;

import "google/protobuf/duration.proto";

service Speech {
    rpc Recognize(RecognizeRequest) returns (RecognizeResponse);

    // rpc StreamingRecognize(stream StreamingRecognizeRequest) returns (stream StreamingRecognizeResponse);
}

// The top-level message sent by the client for the `Recognize` method.
message RecognizeRequest {
  // *Required* Provides information to the recognizer that specifies how to
  // process the request.
  RecognitionConfig config = 1;

  // *Required* The audio data to be recognized.
  RecognitionAudio audio = 2;
}

// Provides information to the recognizer that specifies how to process the
// request.
message RecognitionConfig {
  // Audio encoding of the data sent in the audio message. All encodings support
  // only 1 channel (mono) audio. Only `FLAC` and `WAV` include a header that
  // describes the bytes of audio that follow the header. The other encodings
  // are raw audio bytes with no header.
  //
  // For best results, the audio source should be captured and transmitted using
  // a lossless encoding (`FLAC` or `LINEAR16`). Recognition accuracy may be
  // reduced if lossy codecs, which include the other codecs listed in
  // this section, are used to capture or transmit the audio, particularly if
  // background noise is present.
  enum AudioEncoding {
    // Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
    ENCODING_UNSPECIFIED = 0;

    // Uncompressed 16-bit signed little-endian samples (Linear PCM).
    LINEAR16 = 1;

    // [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    // Codec) is the recommended encoding because it is
    // lossless--therefore recognition is not compromised--and
    // requires only about half the bandwidth of `LINEAR16`. `FLAC` stream
    // encoding supports 16-bit and 24-bit samples, however, not all fields in
    // `STREAMINFO` are supported.
    FLAC = 2;
  }

  // *Required* Encoding of audio data sent in all `RecognitionAudio` messages.
  AudioEncoding encoding = 1;

  // *Required* Sample rate in Hertz of the audio data sent in all
  // `RecognitionAudio` messages. Valid values are: 8000-48000.
  // 16000 is optimal. For best results, set the sampling rate of the audio
  // source to 16000 Hz. If that's not possible, use the native sample rate of
  // the audio source (instead of re-sampling).
  int32 sample_rate_hertz = 2;

  // *Required* The language of the supplied audio as a
  // [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  // Example: "en-US".
  // See [Language Support](https://cloud.google.com/speech/docs/languages)
  // for a list of the currently supported language codes.
  string language_code = 3;

  // *Optional* Maximum number of recognition hypotheses to be returned.
  // Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  // within each `SpeechRecognitionResult`.
  // The server may return fewer than `max_alternatives`.
  // Valid values are `0`-`30`. A value of `0` or `1` will return a maximum of
  // one. If omitted, will return a maximum of one.
  int32 max_alternatives = 4;

  // *Optional* If `true`, the top result includes a list of words and
  // the start and end time offsets (timestamps) for those words. If
  // `false`, no word-level time offset information is returned. The default is
  // `false`.
  bool enable_word_time_offsets = 8;
}

// Contains audio data in the encoding specified in the `RecognitionConfig`.
// Either `content` or `uri` must be supplied. Supplying both or neither
// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]. See
// [audio limits](https://cloud.google.com/speech/limits#content).
message RecognitionAudio {
  // The audio source, which is either inline content or a Google Cloud
  // Storage uri.
  oneof audio_source {
    // The audio data bytes encoded as specified in
    // `RecognitionConfig`. Note: as with all bytes fields, protobuffers use a
    // pure binary representation, whereas JSON representations use base64.
    bytes content = 1;

    // URI that points to a file that contains audio data bytes as specified in
    // `RecognitionConfig`. Currently, only http URIs are supported.
    string uri = 2;
  }
}

// The only message returned to the client by the `Recognize` method. It
// contains the result as zero or more sequential `SpeechRecognitionResult`
// messages.
message RecognizeResponse {
  // *Output-only* Sequential list of transcription results corresponding to
  // sequential portions of audio.
  repeated SpeechRecognitionResult results = 2;
}

// A speech recognition result corresponding to a portion of the audio.
message SpeechRecognitionResult {
  // *Output-only* May contain one or more recognition hypotheses (up to the
  // maximum specified in `max_alternatives`).
  // These alternatives are ordered in terms of accuracy, with the top (first)
  // alternative being the most probable, as ranked by the recognizer.
  repeated SpeechRecognitionAlternative alternatives = 1;
}

// Alternative hypotheses (a.k.a. n-best list).
message SpeechRecognitionAlternative {
  // *Output-only* Transcript text representing the words that the user spoke.
  string transcript = 1;

  // *Output-only* The confidence estimate between 0.0 and 1.0. A higher number
  // indicates an estimated greater likelihood that the recognized words are
  // correct. This field is typically provided only for the top hypothesis, and
  // only for `is_final=true` results. Clients should not rely on the
  // `confidence` field as it is not guaranteed to be accurate or consistent.
  // The default of 0.0 is a sentinel value indicating `confidence` was not set.
  float confidence = 2;

  // *Output-only* A list of word-specific information for each recognized word.
  repeated WordInfo words = 3;
}

// Word-specific information for recognized words. Word information is only
// included in the response when certain request parameters are set, such
// as `enable_word_time_offsets`.
message WordInfo {
  // *Output-only* Time offset relative to the beginning of the audio,
  // and corresponding to the start of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  // This is an experimental feature and the accuracy of the time offset can
  // vary.
  google.protobuf.Duration start_time = 1;

  // *Output-only* Time offset relative to the beginning of the audio,
  // and corresponding to the end of the spoken word.
  // This field is only set if `enable_word_time_offsets=true` and only
  // in the top hypothesis.
  // This is an experimental feature and the accuracy of the time offset can
  // vary.
  google.protobuf.Duration end_time = 2;

  // *Output-only* The word corresponding to this set of information.
  string word = 3;
}